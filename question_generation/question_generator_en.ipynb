{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:24.783153Z",
     "start_time": "2024-03-27T13:54:14.848935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Will\n",
      "[nltk_data]     Blanton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "e:\\VS_Projects\\Q-A-5525\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from qa_evaluation import QA_Evaluator\n",
    "from question_gen_en import QuestionGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:29.917925Z",
     "start_time": "2024-03-27T13:54:29.908111Z"
    }
   },
   "outputs": [],
   "source": [
    "def readable_print(text):\n",
    "    # Replace each period with a period followed by a newline character\n",
    "    modified_text = text.replace('. ', '.\\n')\n",
    "    print(modified_text)\n",
    "\n",
    "def meteor_comparison(generated_questions: list[str], dataset_questions: list[str]):\n",
    "    \"\"\"\n",
    "    Compare the generated questions with the dataset questions using METEOR score\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for gq in generated_questions:\n",
    "        generated_question = word_tokenize(gq.lower())\n",
    "\n",
    "        # tokenize the questions from the dataset\n",
    "        ref_questions = [word_tokenize(ref_q.lower()) for ref_q in dataset_questions]\n",
    "\n",
    "        score = meteor_score(ref_questions, generated_question)\n",
    "\n",
    "        score = score if score >= .00001 else 0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Get Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:30.120182Z",
     "start_time": "2024-03-27T13:54:29.917925Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = pd.read_json(\"../data/xquad.en.json\")\n",
    "\n",
    "articles = [a for a in articles[\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:30.121188Z",
     "start_time": "2024-03-27T13:54:30.101054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How many points did the Panthers defense surrender?', '308'),\n",
       " ('How many career sacks did Jared Allen have?', '136'),\n",
       " ('How many tackles did Luke Kuechly register?', '118'),\n",
       " ('How many balls did Josh Norman intercept?', 'four'),\n",
       " ('Who registered the most sacks on the team this season?', 'Kawann Short'),\n",
       " ('How many interceptions are the Panthers defense credited with in 2015?',\n",
       "  '24'),\n",
       " ('Who led the Panthers in sacks?', 'Kawann Short'),\n",
       " ('How many Panthers defense players were selected for the Pro Bowl?', 'four'),\n",
       " ('How many forced fumbles did Thomas Davis have?', 'four'),\n",
       " ('Which player had the most interceptions for the season?', 'Kurt Coleman'),\n",
       " (\"How many 2015 season interceptions did the Panthers' defense get?\", '24'),\n",
       " ('Who had five sacks in nine games as a Carolina Panthers starter?',\n",
       "  'Kony Ealy'),\n",
       " (\"Who was the Panthers' tackle leader for 2015?\", 'Luke Kuechly.'),\n",
       " ('How many interceptions did Josh Norman score touchdowns with in 2015?',\n",
       "  'two.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crete a list of tuples, each tuple contains the title, context and question\n",
    "\n",
    "cq_pairs = {}\n",
    "for a in articles:\n",
    "    title = a[\"title\"]\n",
    "    for p in a[\"paragraphs\"]:\n",
    "        context = p[\"context\"]\n",
    "        cq_pairs[context] = [(qas[\"question\"], qas[\"answers\"][0][\"text\"])for qas in p[\"qas\"]]   \n",
    "            \n",
    "cq_pairs[list(cq_pairs.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:30.122196Z",
     "start_time": "2024-03-27T13:54:30.104978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cq_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models for Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:30.122196Z",
     "start_time": "2024-03-27T13:54:30.109278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections.\n",
      "Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two.\n",
      "Fellow lineman Mario Addison added 6½ sacks.\n",
      "The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts.\n",
      "Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly.\n",
      "Davis compiled 5½ sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own.\n",
      "Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns.\n"
     ]
    }
   ],
   "source": [
    "context = list(cq_pairs.keys())[0]\n",
    "readable_print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:54:36.508465Z",
     "start_time": "2024-03-27T13:54:30.115865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# generate questions\n",
    "q_gen = QuestionGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:55:10.001252Z",
     "start_time": "2024-03-27T13:55:09.974761Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How many points did the Panthers defense surrender?', '308'),\n",
       " ('How many career sacks did Jared Allen have?', '136'),\n",
       " ('How many tackles did Luke Kuechly register?', '118'),\n",
       " ('How many balls did Josh Norman intercept?', 'four'),\n",
       " ('Who registered the most sacks on the team this season?', 'Kawann Short'),\n",
       " ('How many interceptions are the Panthers defense credited with in 2015?',\n",
       "  '24'),\n",
       " ('Who led the Panthers in sacks?', 'Kawann Short'),\n",
       " ('How many Panthers defense players were selected for the Pro Bowl?', 'four'),\n",
       " ('How many forced fumbles did Thomas Davis have?', 'four'),\n",
       " ('Which player had the most interceptions for the season?', 'Kurt Coleman'),\n",
       " (\"How many 2015 season interceptions did the Panthers' defense get?\", '24'),\n",
       " ('Who had five sacks in nine games as a Carolina Panthers starter?',\n",
       "  'Kony Ealy'),\n",
       " (\"Who was the Panthers' tackle leader for 2015?\", 'Luke Kuechly.'),\n",
       " ('How many interceptions did Josh Norman score touchdowns with in 2015?',\n",
       "  'two.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset questions\n",
    "cq_pairs[context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many points did the Panthers defense surrender?', '308')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_qa = cq_pairs[context][0]\n",
    "example_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many points did the Panthers defense give up?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_q = q_gen.generate_question(\n",
    "    answer=example_qa[1],\n",
    "    context=context)\n",
    "gen_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = QA_Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'308'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.answer_question(context, gen_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:28:23.893746Z",
     "start_time": "2024-03-27T13:24:15.600721Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "score_dfs = []\n",
    "\n",
    "eval_pairs = list(cq_pairs.items())\n",
    "#eval_pairs = random.sample(list(cq_pairs.items()), 4)\n",
    "\n",
    "# for each context and answer, generate a question and evaluate it\n",
    "for (context, qas) in eval_pairs:\n",
    "    for question, answer in qas:\n",
    "\n",
    "        # evaluation metrics to save\n",
    "        generated_q = q_gen.generate_question(answer, context)\n",
    "        meteor_q = meteor_comparison([generated_q], [question])\n",
    "        similarity = evaluator.calculate_similarity(generated_q, question)\n",
    "\n",
    "        # answer the generated question as part of question evaluation\n",
    "        generated_a, answer_similarity = evaluator.evaluate_qa_pair(context, generated_q, answer)\n",
    "        meteor_a = meteor_comparison([generated_a], [answer])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"context\": [context],\n",
    "            \"answer\": [answer],\n",
    "            \"target_q\": [question],\n",
    "            \"generated_q\": [generated_q],\n",
    "            \"generated_q_answer\": [generated_a],\n",
    "            \"exact_match\": [int(generated_a.lower() == answer.lower())],\n",
    "            \"q_METEOR_score\": [meteor_q],\n",
    "            \"a_METEOR_score\": [meteor_a],\n",
    "            \"question_similarity\": [similarity],\n",
    "            \"answer_similarity\": [answer_similarity]\n",
    "        })\n",
    "\n",
    "    score_dfs.append(df)\n",
    "\n",
    "score_df = pd.concat(score_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:44:26.145103Z",
     "start_time": "2024-03-27T13:44:26.122377Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>target_q</th>\n",
       "      <th>generated_q</th>\n",
       "      <th>generated_q_answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>q_METEOR_score</th>\n",
       "      <th>a_METEOR_score</th>\n",
       "      <th>question_similarity</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The University is organized into eleven separa...</td>\n",
       "      <td>Harvard Yard</td>\n",
       "      <td>What is the name of the area that the main cam...</td>\n",
       "      <td>Where is the main campus of Harvard located?</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646834</td>\n",
       "      <td>0.507282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As northwest Europe slowly began to warm up fr...</td>\n",
       "      <td>9000 BP</td>\n",
       "      <td>When was Europe fully forested and recovered f...</td>\n",
       "      <td>By what year was Europe fully forested?</td>\n",
       "      <td>9000 BP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.831209</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The historian Frederick W. Mote wrote that the...</td>\n",
       "      <td>lived in poverty and were ill treated</td>\n",
       "      <td>There were many Mongols with what unexpected s...</td>\n",
       "      <td>What was the status of the Mongol and Semu?</td>\n",
       "      <td>Poor and ill treated</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.381426</td>\n",
       "      <td>0.733110</td>\n",
       "      <td>0.785776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This projection was not included in the final ...</td>\n",
       "      <td>\"Variations of Snow and Ice in the past and at...</td>\n",
       "      <td>What report had the correct date?</td>\n",
       "      <td>What is the name of the ICSI report?</td>\n",
       "      <td>Variations of Snow and Ice in the past and at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.850960</td>\n",
       "      <td>0.494314</td>\n",
       "      <td>0.818869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The University is organized into eleven separa...   \n",
       "1  As northwest Europe slowly began to warm up fr...   \n",
       "2  The historian Frederick W. Mote wrote that the...   \n",
       "3  This projection was not included in the final ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                       Harvard Yard   \n",
       "1                                            9000 BP   \n",
       "2              lived in poverty and were ill treated   \n",
       "3  \"Variations of Snow and Ice in the past and at...   \n",
       "\n",
       "                                            target_q  \\\n",
       "0  What is the name of the area that the main cam...   \n",
       "1  When was Europe fully forested and recovered f...   \n",
       "2  There were many Mongols with what unexpected s...   \n",
       "3                  What report had the correct date?   \n",
       "\n",
       "                                    generated_q  \\\n",
       "0  Where is the main campus of Harvard located?   \n",
       "1       By what year was Europe fully forested?   \n",
       "2   What was the status of the Mongol and Semu?   \n",
       "3          What is the name of the ICSI report?   \n",
       "\n",
       "                                  generated_q_answer  exact_match  \\\n",
       "0                                          Cambridge            0   \n",
       "1                                            9000 BP            1   \n",
       "2                               Poor and ill treated            0   \n",
       "3  Variations of Snow and Ice in the past and at ...            0   \n",
       "\n",
       "   q_METEOR_score  a_METEOR_score  question_similarity  answer_similarity  \n",
       "0        0.334060        0.000000             0.646834           0.507282  \n",
       "1        0.387200        0.937500             0.831209           1.000000  \n",
       "2        0.219780        0.381426             0.733110           0.785776  \n",
       "3        0.277778        0.850960             0.494314           0.818869  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.reset_index(drop=True, inplace=True)\n",
    "score_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>target_q</th>\n",
       "      <th>generated_q</th>\n",
       "      <th>generated_q_answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>q_METEOR_score</th>\n",
       "      <th>a_METEOR_score</th>\n",
       "      <th>question_similarity</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The University is organized into eleven separa...</td>\n",
       "      <td>Harvard Yard</td>\n",
       "      <td>What is the name of the area that the main cam...</td>\n",
       "      <td>Where is the main campus of Harvard located?</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>0</td>\n",
       "      <td>0.334060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646834</td>\n",
       "      <td>0.507282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As northwest Europe slowly began to warm up fr...</td>\n",
       "      <td>9000 BP</td>\n",
       "      <td>When was Europe fully forested and recovered f...</td>\n",
       "      <td>By what year was Europe fully forested?</td>\n",
       "      <td>9000 BP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.831209</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The historian Frederick W. Mote wrote that the...</td>\n",
       "      <td>lived in poverty and were ill treated</td>\n",
       "      <td>There were many Mongols with what unexpected s...</td>\n",
       "      <td>What was the status of the Mongol and Semu?</td>\n",
       "      <td>Poor and ill treated</td>\n",
       "      <td>0</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.381426</td>\n",
       "      <td>0.733110</td>\n",
       "      <td>0.785776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This projection was not included in the final ...</td>\n",
       "      <td>\"Variations of Snow and Ice in the past and at...</td>\n",
       "      <td>What report had the correct date?</td>\n",
       "      <td>What is the name of the ICSI report?</td>\n",
       "      <td>Variations of Snow and Ice in the past and at ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.850960</td>\n",
       "      <td>0.494314</td>\n",
       "      <td>0.818869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  The University is organized into eleven separa...   \n",
       "1  As northwest Europe slowly began to warm up fr...   \n",
       "2  The historian Frederick W. Mote wrote that the...   \n",
       "3  This projection was not included in the final ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                       Harvard Yard   \n",
       "1                                            9000 BP   \n",
       "2              lived in poverty and were ill treated   \n",
       "3  \"Variations of Snow and Ice in the past and at...   \n",
       "\n",
       "                                            target_q  \\\n",
       "0  What is the name of the area that the main cam...   \n",
       "1  When was Europe fully forested and recovered f...   \n",
       "2  There were many Mongols with what unexpected s...   \n",
       "3                  What report had the correct date?   \n",
       "\n",
       "                                    generated_q  \\\n",
       "0  Where is the main campus of Harvard located?   \n",
       "1       By what year was Europe fully forested?   \n",
       "2   What was the status of the Mongol and Semu?   \n",
       "3          What is the name of the ICSI report?   \n",
       "\n",
       "                                  generated_q_answer  exact_match  \\\n",
       "0                                          Cambridge            0   \n",
       "1                                            9000 BP            1   \n",
       "2                               Poor and ill treated            0   \n",
       "3  Variations of Snow and Ice in the past and at ...            0   \n",
       "\n",
       "   q_METEOR_score  a_METEOR_score  question_similarity  answer_similarity  \n",
       "0        0.334060        0.000000             0.646834           0.507282  \n",
       "1        0.387200        0.937500             0.831209           1.000000  \n",
       "2        0.219780        0.381426             0.733110           0.785776  \n",
       "3        0.277778        0.850960             0.494314           0.818869  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:37:12.109404Z",
     "start_time": "2024-03-27T13:37:12.086785Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact_match</th>\n",
       "      <th>q_METEOR_score</th>\n",
       "      <th>a_METEOR_score</th>\n",
       "      <th>question_similarity</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.304704</td>\n",
       "      <td>0.542472</td>\n",
       "      <td>0.676367</td>\n",
       "      <td>0.777982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.072121</td>\n",
       "      <td>0.436434</td>\n",
       "      <td>0.142842</td>\n",
       "      <td>0.203554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494314</td>\n",
       "      <td>0.507282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.263278</td>\n",
       "      <td>0.286070</td>\n",
       "      <td>0.608704</td>\n",
       "      <td>0.716152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.305919</td>\n",
       "      <td>0.616193</td>\n",
       "      <td>0.689972</td>\n",
       "      <td>0.802323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.347345</td>\n",
       "      <td>0.872595</td>\n",
       "      <td>0.757635</td>\n",
       "      <td>0.864152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.831209</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       exact_match  q_METEOR_score  a_METEOR_score  question_similarity  \\\n",
       "count         4.00        4.000000        4.000000             4.000000   \n",
       "mean          0.25        0.304704        0.542472             0.676367   \n",
       "std           0.50        0.072121        0.436434             0.142842   \n",
       "min           0.00        0.219780        0.000000             0.494314   \n",
       "25%           0.00        0.263278        0.286070             0.608704   \n",
       "50%           0.00        0.305919        0.616193             0.689972   \n",
       "75%           0.25        0.347345        0.872595             0.757635   \n",
       "max           1.00        0.387200        0.937500             0.831209   \n",
       "\n",
       "       answer_similarity  \n",
       "count           4.000000  \n",
       "mean            0.777982  \n",
       "std             0.203554  \n",
       "min             0.507282  \n",
       "25%             0.716152  \n",
       "50%             0.802323  \n",
       "75%             0.864152  \n",
       "max             1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49431413"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_semantic = score_df.loc[score_df['question_similarity'].idxmin()]\n",
    "worst_semantic['question_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This projection was not included in the final summary for policymakers.\n",
      "The IPCC has since acknowledged that the date is incorrect, while reaffirming that the conclusion in the final summary was robust.\n",
      "They expressed regret for \"the poor application of well-established IPCC procedures in this instance\".\n",
      "The date of 2035 has been correctly quoted by the IPCC from the WWF report, which has misquoted its own source, an ICSI report \"Variations of Snow and Ice in the past and at present on a Global and Regional Scale\".\n"
     ]
    }
   ],
   "source": [
    "readable_print(worst_semantic['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What report had the correct date?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_semantic['target_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the name of the ICSI report?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_semantic['generated_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Variations of Snow and Ice in the past and at present on a Global and Regional Scale\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_semantic['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Variations of Snow and Ice in the past and at present on a Global and Regional Scal'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_semantic['generated_q_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:39:23.946757Z",
     "start_time": "2024-03-27T13:39:23.934999Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The historian Frederick W.\n",
      "Mote wrote that the usage of the term \"social classes\" for this system was misleading and that the position of people within the four-class system was not an indication of their actual social power and wealth, but just entailed \"degrees of privilege\" to which they were entitled institutionally and legally, so a person's standing within the classes was not a guarantee of their standing, since there were rich and well socially standing Chinese while there were less rich Mongol and Semu than there were Mongol and Semu who lived in poverty and were ill treated.\n"
     ]
    }
   ],
   "source": [
    "best_score = score_df.loc[score_df['q_METEOR_score'].idxmin()]\n",
    "readable_print(best_score['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:39:42.172189Z",
     "start_time": "2024-03-27T13:39:42.146757Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What was the status of the Mongol and Semu?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score['generated_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T13:39:56.630018Z",
     "start_time": "2024-03-27T13:39:56.597264Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There were many Mongols with what unexpected status?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score['target_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lived in poverty and were ill treated'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poor and ill treated'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score['generated_q_answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
